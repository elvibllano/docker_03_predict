# -*- coding: utf-8 -*-
# """.ipynb

# Automatically generated by Colaboratory.

# Original file is located at
#     https://colab.research.google.com/drive/1vppfbxxTTU19_Vf63-G0s5hkBCX79TMc

# ## Mounting folders in Google Drive
# """

# # from google.colab import drive
# # drive.mount('/content/drive')

# """##Importing Libraries"""

import joblib
import json
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import os
import random
import psycopg2
import tensorflow as tf
from keras.utils.generic_utils import get_custom_objects
from keras.models import load_model
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Dropout
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import AveragePooling1D
from keras.layers.convolutional import MaxPooling1D
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import plot_confusion_matrix
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report

tmp_path = "/tmp"

# file = os.getcwd()+"/ElevationValues_full.json"
# file2 = os.getcwd()+"/ElevationValues_full_ResultD2.json"
# file_get_db = os.getcwd()+"/ElevationValues_full_prova.json"
# output_file = os.getcwd()+"/elevation_outdata.txt"
# output_file_prova = os.getcwd()+"/elevation_outdata_prova.txt"

input_file = tmp_path + "/ElevationValues_full.json"
output_file = tmp_path + "/elevation_outdata.txt"

connection_string = "postgresql://postgres:sinergia@172.17.0.2:5432/prova_gis"

#def predict_all():
def predict_all(fileIn, fileOut):
  """## Full Dataset Preprocessing"""
  profiles = []
  with open(fileIn) as file:
  #with open(os.getcwd()+"/ElevationValues_full.json") as file:
  #with open("/content/drive/My Drive/Colab Notebooks/TESI/Dati/Elevation_Volo5_Gaeta/ElevationValues_full.json") as file:
    data = json.load(file)
    for item in data:
      profiles.append(data[item][0])


  print('numero di profili caricati: '+str(len(profiles)))

  l = []
  for item in profiles:
    l.append(len(item[0]))

  n_steps = 42
  n_features = 1
  n_data = len(profiles)

  X_data = np.zeros((n_data,n_steps))
  row = 0

  for item in profiles:
    l_tmp = len(item[0])
    delta = l_tmp - n_steps    
    if(l_tmp > n_steps):    
      if((delta % 2) == 0):
        start = int(delta/2)
        stop = l_tmp - start
      else:
        start = int(delta/2)
        stop = l_tmp - start - 1
      #print(f'{l_tmp} {delta} {start} {stop} {len(range(start,stop))}')
      sub_list = item[0][start:stop]
    elif(l_tmp < 42):
      n_elements = int(abs(delta/2))
      if((delta % 2) == 0):      
        l_head = [ item[0][0] ] * n_elements
        l_stop = [ item[0][-1] ] * n_elements
      else:  
        l_head = [ item[0][0] ] * n_elements
        l_stop = [ item[0][-1] ] * (n_elements + 1)
        sub_list = l_head +  item[0] + l_stop
      #print(len(l_head) + len(l_stop) + l_tmp)
    else:
      sub_list = item[0]
    X_data[row,:] = sub_list - np.min(sub_list)
    #X_data[row,:] = [number - np.min(X_data[row,:]) for number in X_data[row,:]] # subtract minimum from array, so that data is normalized (from 0 to max elevation)
    row+=1

  """## Full Dataset Evaluation"""

  class BalancedSparseCategoricalAccuracy(tf.keras.metrics.SparseCategoricalAccuracy):
      def __init__(self, name='accuracy', dtype=None):
          super().__init__(name, dtype=dtype)
      def update_state(self, y_true, y_pred, sample_weight=None):
          y_flat = y_true
          if y_true.shape.ndims == y_pred.shape.ndims:
              y_flat = tf.squeeze(y_flat, axis=[-1])
          y_true_int = tf.cast(y_flat, tf.int32)
          cls_counts = tf.math.bincount(y_true_int)
          cls_counts = tf.math.reciprocal_no_nan(tf.cast(cls_counts, self.dtype))
          weight = tf.gather(cls_counts, y_true_int)
          return super().update_state(y_true, y_pred, sample_weight=weight)

  # load the same model created before in this Colab
  model1 = tf.keras.models.load_model(os.getcwd()+"/model5Gaeta.h5", 
  #model1 = tf.keras.models.load_model("/content/drive/My Drive/Colab Notebooks/TESI/Dati/model5Gaeta.h5", 
                                      custom_objects={'BalancedSparseCategoricalAccuracy': BalancedSparseCategoricalAccuracy})
  # make predictions on the entire dataset
  y_labels = model1.predict(X_data)
  #print(X_data)
  #print(y_labels)

  classified_fulldata = np.argmax(y_labels, axis=-1) # make an array extracting index of max value into each element of initial array 
  #print((np.array(classified_fulldata)).size)

  # connect predictions with outputs
  out_data = np.zeros((len(X_data),2),dtype=np.uint32) 
  out_data[:,0] = np.arange(1,len(X_data)+1)
  out_data[:,1] = np.array(classified_fulldata)

  #print(len(out_data))
  #print(out_data)

  #np.savetxt("/content/drive/My Drive/Colab Notebooks/TESI/Dati/Elevation_Volo5_Gaeta/elevation_outdata.txt",out_data,fmt="%d")
  np.savetxt(fileOut,out_data,fmt="%d")

  ##### <- da cancellare perchÃ¨ parte del codice del training modello che non ci serve
  # """**Results** of Training"""

  # results = confusion_matrix(np.array(labels), np.array(classified_fulldata))

  # print('Numero di elementi presenti nel test di uscita:  '+str(len(y_labels)))

  # print(''); print('Report sulla classificazione :')
  # print(classification_report(labels, classified_fulldata))

  # disp = ConfusionMatrixDisplay(confusion_matrix=results)
  # disp.plot()
  ##### 
 
def insert_db(idRun, pgs_credentials):
  # connect to the db
  connection = psycopg2.connect(
      database=pgs_credentials[0],
      user=pgs_credentials[1],
      password=pgs_credentials[2],
      host=pgs_credentials[3],
      port=pgs_credentials[4]
  )
  print("Connected...")    
  cursor = connection.cursor()
  print("Cursor obtained...")
  with open(output_file) as file:
    for line in file:
      words = line.split()
      print(words[0])
      print(words[1])
      cursor.execute("UPDATE dlproc.outputlines SET stato = %s,confidence=%s WHERE id_segmento = %s AND id_run = %s", (words[1],0.8,words[0],idRun))
      connection.commit()
  connection.commit()
  cursor.close()
  connection.close()

def get_file_from_db(file, idRun, pgs_credentials):
  # connect to the db
  connection = psycopg2.connect(
      database=pgs_credentials[0],
      user=pgs_credentials[1],
      password=pgs_credentials[2],
      host=pgs_credentials[3],
      port=pgs_credentials[4]
  )
  print("Connected...")    
  cursor = connection.cursor()
  print("Cursor obtained...")
  with open(file,'wb') as f:
    cursor.execute("SELECT dati FROM dlproc.jsonfiles WHERE id_run=%s",(idRun))
    file_data = cursor.fetchone()[0]
    f.write(file_data)
  connection.commit()
  cursor.close()
  connection.close()
  return

def run(run, pgs_credentials):
  get_file_from_db(input_file, run, pgs_credentials)

  predict_all(input_file, output_file)

  insert_db(run,pgs_credentials)
